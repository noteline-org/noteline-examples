{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install noteline-kf==0.2.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from noteline.kf import noteline_nb_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Kubeflow Pipelines inverse proxy host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFP_HOST = !kubectl describe configmap inverse-proxy-config -n default | grep googleusercontent.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(KFP_HOST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add notebooks and set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change bucket name here and in the notebooks\n",
    "NOTEBOOKS = [\"train\", \"clean\"]\n",
    "\n",
    "GCS_BUCKET_NAME = \"\" # set bucket name\n",
    "KFP_URL = KFP_HOST[0]\n",
    "PIPELINE_FILE_NAME = \"pipeline.tar.gz\"\n",
    "PIPELINE_NAME = \"notebooks-pipeline-demo\"\n",
    "EXPERIMENT_NAME = \"pipeline-experiment\"\n",
    "RUN_NAME = \"pipeline-run\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this to copy the notebooks to GCS and create the metadata for the container op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = {}\n",
    "for notebook in NOTEBOOKS:\n",
    "    NOTEBOOK_GCS_PATH = \"gs://{}/{}.ipynb\".format(GCS_BUCKET_NAME, notebook)\n",
    "    NOTEBOOK_OUT_GCS_PATH = NOTEBOOK_GCS_PATH.replace(\".ipynb\", \"-out.ipynb\")\n",
    "    nb_op = noteline_nb_op.NotelineNbOp(notebook_in=NOTEBOOK_GCS_PATH,\n",
    "                                           notebook_out=NOTEBOOK_OUT_GCS_PATH,\n",
    "                                           op_name=\"{}-nb\".format(notebook))\n",
    "    ops[notebook] = {'notebook_in': NOTEBOOK_GCS_PATH,\n",
    "                 'notebook_out': NOTEBOOK_OUT_GCS_PATH,\n",
    "                 'op_name': \"{}-nb\".format(notebook)\n",
    "                }\n",
    "    !gsutil cp ./{notebook}.ipynb gs://{GCS_BUCKET_NAME}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_nb_op(op_metadata):\n",
    "    return noteline_nb_op.NotelineNbOp(notebook_in=op_metadata['notebook_in'],\n",
    "                                           notebook_out= op_metadata['notebook_out'],\n",
    "                                           op_name= op_metadata['op_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pipeline ops and steps based on the notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify as neeeded\n",
    "from kubernetes.client.models import V1EnvVar\n",
    "\n",
    "bucket_env_var = V1EnvVar(name='bucket', value=GCS_BUCKET_NAME)\n",
    "\n",
    "def notebooks_pipeline():\n",
    "    clean_op = _create_nb_op(ops['clean']).add_env_variable(bucket_env_var)\n",
    "    train_op = _create_nb_op(ops['train']).add_env_variable(bucket_env_var).after(clean_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This compiles and uploads the pipelines to the KFP cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(notebooks_pipeline, PIPELINE_FILE_NAME)\n",
    "client = kfp.Client(host=KFP_URL)\n",
    "try:\n",
    "    pipeline_info = client.upload_pipeline(PIPELINE_FILE_NAME, pipeline_name=PIPELINE_NAME)\n",
    "except:\n",
    "    print(\"recreating pipeline\")\n",
    "    client.delete_pipeline(pipeline_info.id)\n",
    "    pipeline_info = client.upload_pipeline(PIPELINE_FILE_NAME, pipeline_name=PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and run the experiment and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new experiment\n",
    "experiment = client.create_experiment(name=EXPERIMENT_NAME)\n",
    "\n",
    "# Create a new run associated with experiment and our uploaded pipeline\n",
    "run = client.run_pipeline(experiment.id, RUN_NAME, pipeline_id=pipeline_info.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
